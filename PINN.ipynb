{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo code for PINN <br>\n",
    "Author: yuanye_zhou@buaa.edu.cn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 基础定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 导入库\n",
    "\n",
    "import paddle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 定义网络\n",
    "\n",
    "class MyNet(paddle.nn.Layer):\n",
    "    def __init__(self,numIN,numOUT,numHidden,numLayer):\n",
    "        super().__init__()\n",
    "        self.IN = paddle.nn.Linear(numIN,numHidden) # 输入层\n",
    "        self.FC_layers = [] # 隐藏层\n",
    "        for i in range(numLayer - 1):\n",
    "            fc_layer = paddle.nn.Linear(numHidden,numHidden)\n",
    "            self.FC_layers.append(fc_layer)\n",
    "        self.OUT = paddle.nn.Linear(numHidden,numOUT)  # 输出层\n",
    "        self.ACT = paddle.nn.functional.tanh\n",
    "    \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        H = inputs\n",
    "        H = self.IN(H)\n",
    "        H = self.ACT(H)\n",
    "        for fc_layer in self.FC_layers:\n",
    "            H = fc_layer(H)\n",
    "            H = self.ACT(H)\n",
    "        outputs = self.OUT(H)      \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 具体案例 \n",
    "（代码执行顺序：0. 重启内核; 1.基础定义; 2.任选一个小节案例代码运行）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 1D poisson 正问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义 DataLoss 和 Equation Loss\n",
    "\n",
    "class DataLoss(paddle.nn.Layer):\n",
    "\n",
    "    def __init__(self,nn_fun):\n",
    "        super().__init__()\n",
    "        self.fun = nn_fun \n",
    "\n",
    "    def forward(self, X, Y_true):\n",
    "        Y_pred = self.fun(X)\n",
    "        loss = paddle.sum(paddle.square(Y_pred - Y_true))\n",
    "        return loss\n",
    "\n",
    "class EqLoss(paddle.nn.Layer):\n",
    "\n",
    "    def __init__(self,nn_fun):\n",
    "        super().__init__()\n",
    "        self.fun = nn_fun\n",
    "\n",
    "    def forward(self, X):\n",
    "        x = X[:,0].reshape([X.shape[0],1])\n",
    "        X = x\n",
    "        u = self.fun(X)\n",
    "        u_x = paddle.grad(u,x,create_graph=True,retain_graph=True)[0]\n",
    "        u_xx = paddle.grad(u_x,x,create_graph=True,retain_graph=True)[0]\n",
    "        eq = np.pi**2*paddle.sin(np.pi*x) + u_xx # poisson equation, u'' = pi^2 * sin(pi*x)\n",
    "        loss = paddle.sum(paddle.square(eq))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 生成训练数据点\n",
    "X = np.linspace(-1,1,100)\n",
    "X = paddle.to_tensor(X,stop_gradient=False,dtype='float32')\n",
    "X = X.reshape([100,1])\n",
    "X_bc = np.linspace(-1,1,2)\n",
    "X_bc = paddle.to_tensor(X_bc,stop_gradient=True,dtype='float32')\n",
    "X_bc = X_bc.reshape([2,1])\n",
    "Y_bc = paddle.zeros([2,1],dtype='float32')\n",
    "\n",
    "# 调用网络\n",
    "numIN = 1\n",
    "numOUT = 1\n",
    "numHidden = 8\n",
    "numLayer = 2\n",
    "nn_fun = MyNet(numIN,numOUT,numHidden,numLayer)\n",
    "\n",
    "# 调用Loss\n",
    "Loss1 = DataLoss(nn_fun)\n",
    "Loss2 = EqLoss(nn_fun)\n",
    "\n",
    "# 调用优化器\n",
    "lr = 0.0001\n",
    "train_parameter = nn_fun.parameters()\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=lr,parameters=train_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 训练网络\n",
    "epoch = 1\n",
    "\n",
    "\n",
    "while (epoch <= 20000):\n",
    "\n",
    "    lossData = Loss1(X_bc,Y_bc)\n",
    "    lossData.backward(retain_graph=True)\n",
    "\n",
    "    lossEQ = Loss2(X)\n",
    "    lossEQ.backward(retain_graph=True)\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.clear_grad()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('epoch:',epoch,'loss_data:',float(lossData),'loss_eq:',float(lossEQ))\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 输出结果\n",
    "\n",
    "Y_pred = nn_fun(X)\n",
    "Y_true = paddle.sin(np.pi*X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 预测值与真实值对比\n",
    "\n",
    "plt.plot(Y_pred)\n",
    "plt.plot(Y_true,marker='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 unsteady Burgers equation 正问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义 DataLoss 和 Equation Loss\n",
    "\n",
    "class DataLoss(paddle.nn.Layer):\n",
    "\n",
    "    def __init__(self,nn_fun):\n",
    "        super().__init__()\n",
    "        self.fun = nn_fun \n",
    "\n",
    "    def forward(self, X, Y_true):\n",
    "        Y_pred = self.fun(X)\n",
    "        loss = paddle.sum(paddle.square(Y_pred - Y_true))\n",
    "        return loss\n",
    "\n",
    "class EqLoss(paddle.nn.Layer):\n",
    "\n",
    "    def __init__(self,nn_fun,miu):\n",
    "        super().__init__()\n",
    "        self.fun = nn_fun\n",
    "        self.miu = miu\n",
    "\n",
    "    def forward(self, X):\n",
    "        t = X[:,0].reshape([X.shape[0],1])\n",
    "        x = X[:,1].reshape([X.shape[0],1])\n",
    "        X = paddle.concat([t,x],axis=-1)\n",
    "        u = self.fun(X)\n",
    "        u_t = paddle.grad(u,t,create_graph=True,retain_graph=True)[0]\n",
    "        u_x = paddle.grad(u,x,create_graph=True,retain_graph=True)[0]\n",
    "        u_xx = paddle.grad(u_x,x,create_graph=True,retain_graph=True)[0]\n",
    "        eq = u_t + u*u_x - self.miu*u_xx # burgers equation, dudt + u*dudx = miu*ddudx\n",
    "        loss = paddle.sum(paddle.square(eq))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 生成训练数据点\n",
    "\n",
    "def genData(t_start,t_end,nt,x_start,x_end,nx):\n",
    "    t = np.linspace(t_start,t_end,nt)\n",
    "    x = np.linspace(x_start,x_end,nx)\n",
    "    T,X = np.meshgrid(t,x)\n",
    "    T = T.reshape([nx*nt,1])\n",
    "    X = X.reshape([nx*nt,1])\n",
    "    T = paddle.to_tensor(T,dtype='float32')\n",
    "    X = paddle.to_tensor(X,dtype='float32')\n",
    "    TX = paddle.concat([T,X],axis=-1)\n",
    "    return TX\n",
    "\n",
    "TX_domain = genData(0,1,100,-1,1,100)\n",
    "TX_bc = genData(0,1,100,-1,1,2)\n",
    "Y_bc = paddle.zeros([2*100,1],dtype='float32')\n",
    "TX_ic = genData(0,0,1,-1,1,100)\n",
    "Y_ic = -paddle.sin(np.pi*TX_ic[:,1:2])\n",
    "\n",
    "TX_domain.stop_gradient = False\n",
    "\n",
    "# 调用网络\n",
    "numIN = 2\n",
    "numOUT = 1\n",
    "numHidden = 48\n",
    "numLayer = 4\n",
    "nn_fun = MyNet(numIN,numOUT,numHidden,numLayer)\n",
    "try:\n",
    "    load_net_params = paddle.load('/home/aistudio/' + 'burgers_params')\n",
    "    nn_fun.set_state_dict(load_net_params)\n",
    "except:\n",
    "    print('no saved params')\n",
    "\n",
    "# 调用Loss\n",
    "miu = 0.01/np.pi\n",
    "Loss1 = DataLoss(nn_fun)\n",
    "Loss2 = EqLoss(nn_fun,miu)\n",
    "\n",
    "# 调用优化器\n",
    "lr = 0.001\n",
    "train_parameter = nn_fun.parameters()\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=lr,parameters=train_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 训练网络\n",
    "epoch = 1\n",
    "\n",
    "while (epoch <= 100000):\n",
    "\n",
    "    lossData_bc = 100*Loss1(TX_bc,Y_bc)\n",
    "    lossData_bc.backward()\n",
    "\n",
    "    lossData_ic = 100*Loss1(TX_ic,Y_ic)\n",
    "    lossData_ic.backward()\n",
    "\n",
    "    lossEQ = Loss2(TX_domain)\n",
    "    lossEQ.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.clear_grad()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('epoch:',epoch,'loss_data_bc:',float(lossData_bc),'loss_data_ic:',float(lossData_ic),'loss_eq:',float(lossEQ))\n",
    "        paddle.save(nn_fun.state_dict(),'/home/aistudio/' + 'burgers_params') \n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 输出结果可视化\n",
    "\n",
    "Y_pred = nn_fun(TX_domain)\n",
    "plt.contourf(Y_pred.reshape([100,100]), 100, cmap='RdBu_r', zorder=1)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TX_test = genData(0,0,1,-1,1,100)\n",
    "plt.plot(TX_test[:,1],nn_fun(TX_test))\n",
    "TX_test = genData(0.1,0.1,1,-1,1,100)\n",
    "plt.plot(TX_test[:,1],nn_fun(TX_test))\n",
    "TX_test = genData(1,1,1,-1,1,100)\n",
    "plt.plot(TX_test[:,1],nn_fun(TX_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 自适应残差采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Residual Based Sampling\n",
    "\n",
    "class Eq(paddle.nn.Layer):\n",
    "\n",
    "    def __init__(self,nn_fun,miu):\n",
    "        super().__init__()\n",
    "        self.fun = nn_fun\n",
    "        self.miu = miu\n",
    "\n",
    "    def forward(self, X):\n",
    "        t = X[:,0].reshape([X.shape[0],1])\n",
    "        x = X[:,1].reshape([X.shape[0],1])\n",
    "        X = paddle.concat([t,x],axis=-1)\n",
    "        u = self.fun(X)\n",
    "        u_t = paddle.grad(u,t,create_graph=True,retain_graph=True)[0]\n",
    "        u_x = paddle.grad(u,x,create_graph=True,retain_graph=True)[0]\n",
    "        u_xx = paddle.grad(u_x,x,create_graph=True,retain_graph=True)[0]\n",
    "        eq = u_t + u*u_x - self.miu*u_xx # burgers equation, dudt + u*dudx = miu*ddudx\n",
    "        return eq\n",
    "\n",
    "\n",
    "calEQ = Eq(nn_fun,miu) \n",
    "\n",
    "# 初始化 sample points\n",
    "\n",
    "TX_sample = genData(0,1,10,-1,1,10)\n",
    "TX_sample.stop_gradient = False\n",
    "\n",
    "epoch = 1\n",
    "while (epoch <= 1000):\n",
    "\n",
    "    lossData_bc = 100*Loss1(TX_bc,Y_bc)\n",
    "    lossData_bc.backward()\n",
    "\n",
    "    lossData_ic = 100*Loss1(TX_ic,Y_ic)\n",
    "    lossData_ic.backward()\n",
    "\n",
    "    lossEQ = Loss2(TX_sample)\n",
    "    lossEQ.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.clear_grad()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('epoch:',epoch,'loss_data_bc:',float(lossData_bc),'loss_data_ic:',float(lossData_ic),'loss_eq:',float(lossEQ))\n",
    "    epoch += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 自适应采样\n",
    "\n",
    "epoch = 1\n",
    "while (epoch <= 20000):\n",
    "\n",
    "    lossData_bc = 100*Loss1(TX_bc,Y_bc)\n",
    "    lossData_bc.backward()\n",
    "\n",
    "    lossData_ic = 100*Loss1(TX_ic,Y_ic)\n",
    "    lossData_ic.backward()\n",
    "\n",
    "    lossEQ = Loss2(TX_sample)\n",
    "    lossEQ.backward(retain_graph=True)\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.clear_grad()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('epoch:',epoch,'loss_data_bc:',float(lossData_bc),'loss_data_ic:',float(lossData_ic),'loss_eq:',float(lossEQ))\n",
    "    epoch += 1\n",
    "\n",
    "    # update sample points every 1000 epoch\n",
    "    if epoch % 1000 == 0:\n",
    "        R = calEQ(TX_domain)\n",
    "        # find large R points\n",
    "        TX_sample_add = paddle.where(paddle.abs(R)>1,TX_domain,paddle.zeros([TX_domain.shape[0],TX_domain.shape[1]]))\n",
    "        # remove zero points\n",
    "        TX_sample_add = paddle.unique(TX_sample_add,axis=0)\n",
    "        TX_sample = paddle.concat([TX_sample,TX_sample_add],axis=0)      \n",
    "        print('number of sample points:',TX_sample.shape[0])  \n",
    "        (R-R).backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 输出结果可视化\n",
    "\n",
    "Y_pred = nn_fun(TX_domain)\n",
    "plt.contourf(Y_pred.reshape([100,100]), 100, cmap='RdBu_r', zorder=1)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TX_test = genData(0,0,1,-1,1,100)\n",
    "plt.plot(TX_test[:,1],nn_fun(TX_test))\n",
    "TX_test = genData(0.1,0.1,1,-1,1,100)\n",
    "plt.plot(TX_test[:,1],nn_fun(TX_test))\n",
    "TX_test = genData(1,1,1,-1,1,100)\n",
    "plt.plot(TX_test[:,1],nn_fun(TX_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 1D 反问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 定义 DataLoss 和 Equation Loss\n",
    "\n",
    "class DataLoss(paddle.nn.Layer):\n",
    "\n",
    "    def __init__(self,nn_fun):\n",
    "        super().__init__()\n",
    "        self.fun = nn_fun \n",
    "\n",
    "    def forward(self, X, Y_true):\n",
    "        Y_pred = self.fun(X)\n",
    "        loss = paddle.sum(paddle.square(Y_pred - Y_true))\n",
    "        return loss\n",
    "\n",
    "class EqLoss(paddle.nn.Layer):\n",
    "\n",
    "    def __init__(self,nn_fun,gamma):\n",
    "        super().__init__()\n",
    "        self.fun = nn_fun\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, X):\n",
    "        x = X[:,0].reshape([X.shape[0],1])\n",
    "        X = x\n",
    "        u = self.fun(X)\n",
    "        u_x = paddle.grad(u,x,create_graph=True,retain_graph=True)[0]\n",
    "        eq = u_x - self.gamma*(1+np.pi/2*paddle.cos(np.pi/2*x)) # 1D equation, u' = gamma * (1 + pi/2*cos(pi/2*x))\n",
    "        loss = paddle.sum(paddle.square(eq))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 生成训练数据点\n",
    "X = np.linspace(0,10,100)\n",
    "X = paddle.to_tensor(X,stop_gradient=False,dtype='float32')\n",
    "X = X.reshape([100,1])\n",
    "X_sup = np.linspace(0,5,10)\n",
    "X_sup = paddle.to_tensor(X_sup,stop_gradient=True,dtype='float32')\n",
    "X_sup = X_sup.reshape([10,1])\n",
    "Y_sup = X_sup + paddle.sin(np.pi/2*X_sup)\n",
    "\n",
    "# 调用网络\n",
    "numIN = 1\n",
    "numOUT = 1\n",
    "numHidden = 48\n",
    "numLayer = 3\n",
    "nn_fun = MyNet(numIN,numOUT,numHidden,numLayer)\n",
    "\n",
    "# 反问题未知参数\n",
    "gamma = paddle.rand([1],dtype='float32')\n",
    "gamma.stop_gradient = False\n",
    "\n",
    "# 调用Loss\n",
    "Loss1 = DataLoss(nn_fun)\n",
    "Loss2 = EqLoss(nn_fun,gamma)\n",
    "\n",
    "# 调用优化器\n",
    "lr = 0.001\n",
    "train_parameter = nn_fun.parameters() + [gamma]\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=lr,parameters=train_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 训练网络\n",
    "epoch = 1\n",
    "\n",
    "while (epoch <= 50000):\n",
    "\n",
    "    lossData = 100*Loss1(X_sup,Y_sup)\n",
    "    lossData.backward(retain_graph=True)\n",
    "\n",
    "    lossEQ = Loss2(X)\n",
    "    lossEQ.backward(retain_graph=True)\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.clear_grad()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('epoch:',epoch,'loss_data:',float(lossData),'loss_eq:',float(lossEQ),'gamma:',float(gamma))\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 输出结果\n",
    "\n",
    "Y_pred = nn_fun(X)\n",
    "Y_true = X + paddle.sin(np.pi/2*X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 预测值与真实值对比\n",
    "\n",
    "plt.plot(Y_pred)\n",
    "plt.plot(Y_true,marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
